#!/usr/bin/env python3
"""
Aratour Issues Fixer - Fix all data quality issues by fetching fresh data from URLs

This script addresses all issues found by analyze_aratur_data.py by:
- Fetching actual HTML from each offer URL
- Re-extracting data using the scraper's logic
- Updating offers with accurate, current information
"""

import json
import re
import asyncio
import aiohttp
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any

async def fetch_and_fix_offer(offer_data: Dict[str, Any], session: aiohttp.ClientSession) -> Dict[str, Any]:
    """Fetch fresh data from offer URL and update the offer."""
    url = offer_data.get('link', '')
    if not url:
        return offer_data

    try:
        print(f"  Fetching: {url}")
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            html = await response.text()

        # Create a temporary offer object for extraction
        temp_offer = {
            'title': offer_data.get('title', ''),
            'link': url,
            'price': '',
            'dates': '',
            'destination': ''
        }

        # Extract data from HTML using the same logic as the scraper
        updated_offer = extract_offer_details_from_html(temp_offer, html)

        # Update original offer with fresh data
        offer_data.update({
            'price': updated_offer.get('price', offer_data.get('price', '')),
            'dates': updated_offer.get('dates', offer_data.get('dates', '')),
            'destination': updated_offer.get('destination', offer_data.get('destination', ''))
        })

        print(f"    Updated: dates='{offer_data['dates']}', dest='{offer_data['destination']}', price='{offer_data['price']}'")
        return offer_data

    except Exception as e:
        print(f"    Error fetching {url}: {e}")
        return offer_data


def extract_offer_details_from_html(offer: Dict[str, Any], html: str) -> Dict[str, Any]:
    """Extract offer details from HTML using the same logic as the scraper."""
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html, 'html.parser')

    # Clean HTML
    for script in soup(["script", "style"]):
        script.decompose()

    # Extract price
    price_patterns = [
        r'(\d{3,5}(?:[.,]\d{2})?)\s*Ð»Ð²',
        r'Ñ†ÐµÐ½Ð° Ð¾Ñ‚\s*(\d{3,5}(?:[.,]\d{2})?)\s*Ð»Ð²',
    ]
    page_text = soup.get_text(separator='\n')
    for pattern in price_patterns:
        match = re.search(pattern, page_text)
        if match:
            offer['price'] = f"{match.group(1)} Ð»Ð²."
            break

    # Extract dates
    if not offer['dates'] or offer['dates'].strip() == "":
        # First try: Find spans with 'Ð´Ð¾' and dates
        date_spans = soup.find_all('span', string=lambda text: text and 'Ð´Ð¾' in text and re.search(r'\d{1,2}[./-]\d{1,2}[./-]\d{4}', text))
        if date_spans:
            date_text = date_spans[0].get_text().strip()
            date_pattern = r'(\d{1,2}[./-]\d{1,2}[./-]\d{4})'
            all_dates = re.findall(date_pattern, date_text)
            if all_dates:
                from datetime import datetime
                parsed_dates = [datetime.strptime(d.replace('-', '.'), "%d.%m.%Y") for d in all_dates]
                parsed_dates.sort()
                first_date = parsed_dates[0].strftime("%d.%m.%Y")
                last_date = parsed_dates[-1].strftime("%d.%m.%Y")
                offer['dates'] = f"{first_date} - {last_date}" if first_date != last_date else first_date
        else:
            # Second try: Calendar spans
            calendar_spans = soup.find_all('span', class_='icon-calendar')
            for calendar_span in calendar_spans:
                parent_div = calendar_span.find_parent('div', class_='offer-info')
                if parent_div:
                    div_text = parent_div.get_text().strip()
                    if re.search(r'\d{1,2}[./-]\d{1,2}[./-]\d{4}', div_text):
                        date_pattern = r'(\d{1,2}[./-]\d{1,2}[./-]\d{4})'
                        all_dates = re.findall(date_pattern, div_text)
                        if all_dates:
                            from datetime import datetime
                            parsed_dates = [datetime.strptime(d.replace('-', '.'), "%d.%m.%Y") for d in all_dates]
                            parsed_dates.sort()
                            first_date = parsed_dates[0].strftime("%d.%m.%Y")
                            last_date = parsed_dates[-1].strftime("%d.%m.%Y")
                            offer['dates'] = f"{first_date} - {last_date}" if first_date != last_date else first_date
                            break

    # If single date and multi-day trip, calculate return date
    if offer['dates'] and '-' not in offer['dates']:
        duration_match = re.search(r'(\d+)\s*Ð´Ð½Ð¸\s*/\s*(\d+)\s*Ð½Ð¾Ñ‰ÑƒÐ²ÐºÐ¸', page_text)
        if duration_match:
            days = int(duration_match.group(1))
            if days > 1:
                from datetime import datetime, timedelta
                try:
                    dep_date = datetime.strptime(offer['dates'], "%d.%m.%Y")
                    ret_date = dep_date + timedelta(days=days - 1)
                    offer['dates'] = f"{offer['dates']} - {ret_date.strftime('%d.%m.%Y')}"
                except ValueError:
                    pass

    # Extract destination
    known_destinations = [
        'Ð¢ÑƒÑ€Ñ†Ð¸Ñ', 'Ð“ÑŠÑ€Ñ†Ð¸Ñ', 'Ð˜Ñ‚Ð°Ð»Ð¸Ñ', 'Ð˜ÑÐ¿Ð°Ð½Ð¸Ñ', 'Ð¤Ñ€Ð°Ð½Ñ†Ð¸Ñ', 'Ð•Ð³Ð¸Ð¿ÐµÑ‚',
        'Ð¢ÑƒÐ½Ð¸Ñ', 'ÐœÐ°Ñ€Ð¾ÐºÐ¾', 'Ð‘ÑŠÐ»Ð³Ð°Ñ€Ð¸Ñ', 'ÐÐ»Ð±Ð°Ð½Ð¸Ñ', 'ÐœÐ°ÐºÐµÐ´Ð¾Ð½Ð¸Ñ', 'Ð¡ÑŠÑ€Ð±Ð¸Ñ',
        'Ð§ÐµÑ€Ð½Ð° Ð³Ð¾Ñ€Ð°', 'Ð¥ÑŠÑ€Ð²Ð°Ñ‚Ð¸Ñ', 'Ð¡Ð»Ð¾Ð²ÐµÐ½Ð¸Ñ', 'ÐÐ²ÑÑ‚Ñ€Ð¸Ñ', 'Ð¨Ð²ÐµÐ¹Ñ†Ð°Ñ€Ð¸Ñ',
        'Ð§ÐµÑ…Ð¸Ñ', 'ÐŸÐ¾Ð»ÑˆÐ°', 'Ð£Ð½Ð³Ð°Ñ€Ð¸Ñ', 'Ð ÑƒÐ¼ÑŠÐ½Ð¸Ñ', 'Ð“ÐµÑ€Ð¼Ð°Ð½Ð¸Ñ', 'Ð¥Ð¾Ð»Ð°Ð½Ð´Ð¸Ñ',
        'Ð‘ÐµÐ»Ð³Ð¸Ñ', 'Ð’ÐµÐ»Ð¸ÐºÐ¾Ð±Ñ€Ð¸Ñ‚Ð°Ð½Ð¸Ñ', 'Ð˜Ñ€Ð»Ð°Ð½Ð´Ð¸Ñ', 'ÐŸÐ¾Ñ€Ñ‚ÑƒÐ³Ð°Ð»Ð¸Ñ', 'Ð™Ð¾Ñ€Ð´Ð°Ð½Ð¸Ñ',
        'ÐšÑƒÐ±Ð°', 'ÐœÐµÐºÑÐ¸ÐºÐ¾', 'Ð”Ð¾Ð¼Ð¸Ð½Ð¸ÐºÐ°Ð½Ð°', 'Ð¯Ð¼Ð°Ð¹ÐºÐ°', 'Ð¢Ð°Ð¹Ð»Ð°Ð½Ð´', 'Ð’Ð¸ÐµÑ‚Ð½Ð°Ð¼',
        'Ð¯Ð¿Ð¾Ð½Ð¸Ñ', 'ÐšÐ¸Ñ‚Ð°Ð¹', 'Ð˜Ð½Ð´Ð¸Ñ', 'Ð˜Ð½Ð´Ð¾Ð½ÐµÐ·Ð¸Ñ', 'ÐœÐ°Ð»Ð°Ð¹Ð·Ð¸Ñ', 'Ð¡Ð¸Ð½Ð³Ð°Ð¿ÑƒÑ€',
        'Ð®Ð¶Ð½Ð° ÐšÐ¾Ñ€ÐµÑ', 'Ð¤Ð¸Ð»Ð¸Ð¿Ð¸Ð½Ð¸', 'ÐÐ²ÑÑ‚Ñ€Ð°Ð»Ð¸Ñ', 'ÐÐ¾Ð²Ð° Ð—ÐµÐ»Ð°Ð½Ð´Ð¸Ñ', 'ÐšÐ°Ð½Ð°Ð´Ð°',
        'Ð¡ÐÐ©', 'Ð‘Ñ€Ð°Ð·Ð¸Ð»Ð¸Ñ', 'ÐÑ€Ð¶ÐµÐ½Ñ‚Ð¸Ð½Ð°', 'Ð§Ð¸Ð»Ð¸', 'ÐŸÐµÑ€Ñƒ', 'ÐšÐ¾Ð»ÑƒÐ¼Ð±Ð¸Ñ',
        'Ð•ÐºÐ²Ð°Ð´Ð¾Ñ€', 'Ð‘Ð¾Ð»Ð¸Ð²Ð¸Ñ', 'Ð£Ñ€ÑƒÐ³Ð²Ð°Ð¹', 'ÐŸÐ°Ñ€Ð°Ð³Ð²Ð°Ð¹', 'ÐœÐ°Ð»Ñ‚Ð°'
    ]

    title_elem = soup.find('title')
    if title_elem:
        title_text = title_elem.get_text().strip()
        destination_patterns = [
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s+\d{4}\s*â€“',
            r'Aratour\s*-\s*([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)',
            r'Ð•ÐºÑÐºÑƒÑ€Ð·Ð¸Ñ\s+Ð´Ð¾\s+([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)',
            r'ÐŸÐ¾Ñ‡Ð¸Ð²ÐºÐ°\s+Ð²\s+([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)',
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s*-\s*Aratour',
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s+Ð˜ÐœÐŸÐ•Ð Ð¡ÐšÐ˜Ð¢Ð•',
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s+â€“\s+ÐœÐ˜Ð¡Ð¢Ð˜ÐšÐ',
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s+â€“\s+Ð—Ð•ÐœÐ¯',
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s+â€“\s+\d+',
            r'([Ð-Ð¯A-Z][Ð°-ÑÐ-Ð¯a-zA-Z\s]+)\s+40\s+ÐÐ®ÐÐÐ¡Ð',
        ]
        for pattern in destination_patterns:
            match = re.search(pattern, title_text, re.IGNORECASE)
            if match:
                extracted_dest = match.group(1).strip()
                if extracted_dest in known_destinations:
                    offer['destination'] = extracted_dest
                    break

    return offer


def load_offers(json_path: str) -> List[Dict[str, Any]]:
    """Load offers from JSON file."""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: {json_path} not found")
        return []


def save_offers(offers: List[Dict[str, Any]], json_path: str) -> None:
    """Save offers to JSON file."""
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(offers, f, ensure_ascii=False, indent=2)
    print(f"âœ“ Saved {len(offers)} offers to {json_path}")


async def fix_offers_with_fresh_data(offers: List[Dict[str, Any]]) -> int:
    """Fix offers by fetching fresh data from their URLs."""
    fixed_count = 0

    async with aiohttp.ClientSession() as session:
        for i, offer in enumerate(offers):
            print(f"Processing offer [{i}]: {offer.get('title', '')[:50]}...")

            # Check if offer needs fixing (has issues)
            needs_fixing = (
                not offer.get('dates') or offer.get('dates', '').strip() == '' or
                not offer.get('destination') or offer.get('destination', '').strip() == '' or
                not offer.get('price') or offer.get('price', '').strip() == ''
            )

            if needs_fixing:
                original_data = {
                    'dates': offer.get('dates', ''),
                    'destination': offer.get('destination', ''),
                    'price': offer.get('price', '')
                }

                updated_offer = await fetch_and_fix_offer(offer, session)

                # Check if data actually changed
                if (updated_offer.get('dates') != original_data['dates'] or
                    updated_offer.get('destination') != original_data['destination'] or
                    updated_offer.get('price') != original_data['price']):
                    fixed_count += 1
                    print(f"  âœ“ Fixed offer [{i}]")
                else:
                    print(f"  - No changes needed for offer [{i}]")
            else:
                print(f"  - Offer [{i}] already complete")

    return fixed_count


def fix_inconsistent_date_ranges(offers: List[Dict[str, Any]]) -> int:
    """Fix offers with inconsistent date ranges."""
    fixed_count = 0

    for i, offer in enumerate(offers):
        dates = offer.get('dates', '').strip()
        title = offer.get('title', '').lower()

        if '-' not in dates:
            continue

        # Extract duration from title
        duration_days = None
        duration_match = re.search(r'(\d+)\s*Ð´Ð½Ð¸', title)
        if duration_match:
            duration_days = int(duration_match.group(1))
        else:
            night_match = re.search(r'(\d+)\s*Ð½Ð¾Ñ‰ÑƒÐ²ÐºÐ¸', title)
            if night_match:
                duration_days = int(night_match.group(1)) + 1

        if not duration_days:
            continue

        try:
            date_parts = dates.split(' - ')
            if len(date_parts) == 2:
                start_date = datetime.strptime(date_parts[0].strip(), "%d.%m.%Y")
                end_date = datetime.strptime(date_parts[1].strip(), "%d.%m.%Y")
                actual_days = (end_date - start_date).days + 1

                # If the range is too short for the duration, extend it
                if actual_days < duration_days and duration_days - actual_days <= 2:  # Allow small adjustments
                    new_end_date = start_date + timedelta(days=duration_days - 1)
                    new_dates = f"{date_parts[0].strip()} - {new_end_date.strftime('%d.%m.%Y')}"
                    offer['dates'] = new_dates
                    fixed_count += 1
                    print(f"  Fixed [{i}]: {dates} -> {new_dates} (duration: {duration_days} days)")
        except ValueError:
            pass  # Invalid date format

    return fixed_count


def fix_invalid_destinations(offers: List[Dict[str, Any]]) -> int:
    """Fix offers with invalid destinations by clearing them so they can be re-extracted."""
    fixed_count = 0

    invalid_keywords = [
        'Ð¿Ð°Ñ€Ñ‚Ð½ÑŒÐ¾Ñ€ÑÑ‚Ð²Ð¾', 'partnership', 'Ð°Ð±Ð°ÐºÑ', 'abaks',
        'pochi', 'ekskurzi', 'tour', 'Ð¿ÑŠÑ‚ÑƒÐ²Ð°Ð½', 'Ð¿ÑŠÑ‚ÐµÑˆÐµÑÑ‚Ð²', 'vacation', 'trip',
        'early', 'booking', 'Ñ€Ð°Ð½Ð½Ð¸', 'Ð·Ð°Ð¿Ð¸ÑÐ²Ð°Ð½', 'Ð»ÑÑ‚Ð¾', 'Ð·Ð¸Ð¼Ð°', 'Ð¿Ñ€Ð¾Ð»ÐµÑ‚', 'ÐµÑÐµÐ½',
        'all', 'inclusive', 'all-inclusive', 'Ð²ÑÐ¸Ñ‡ÐºÐ¾', 'Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½', 'Ð¾Ñ‚', 'Ð´Ð¾', 'Ð²',
        'ÐºÐ¾Ð»ÐµÐ´Ð°', 'christmas', 'Ð½Ð¾Ð²Ð°-Ð³Ð¾Ð´Ð¸Ð½Ð°', 'new-year', 'Ð²ÐµÐ»Ð¸ÐºÐ´ÐµÐ½', 'easter',
        'ÑƒÐ¸ÐºÐµÐ½Ð´', 'weekend', 'ÐµÐºÐ·Ð¾Ñ‚Ð¸Ñ‡Ð½Ð¸', 'exotic', 'ÐºÑ€ÑƒÐ¸Ð·Ð¸', 'cruises',
        'Ð°Ð²Ñ‚Ð¾Ñ€ÑÐºÐ¸', 'author', 'ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð½Ð¸', 'special', 'Ð¿Ñ€Ð¾Ð¼Ð¾', 'promo',
        'Ñ‚Ñ€ÑŠÐ³Ð²Ð°Ð½Ðµ', 'departure', 'Ð²Ð°Ñ€Ð½Ð°', 'sofia', 'ÑÐ¾Ñ„Ð¸Ñ', 'burgas', 'Ð±ÑƒÑ€Ð³Ð°Ñ',
        'Ð¿Ð»Ð¾Ð²Ð´Ð¸Ð²', 'plovdiv', 'Ð¾Ñ‚', 'from', 'Ð»ÐµÑ‚Ð¸Ñ‰Ðµ', 'airport'
    ]

    for i, offer in enumerate(offers):
        destination = offer.get('destination', '').strip()

        if destination:
            dest_lower = destination.lower()
            if any(keyword in dest_lower for keyword in invalid_keywords):
                print(f"  Cleared invalid destination [{i}]: '{destination}' -> ''")
                offer['destination'] = ''
                fixed_count += 1

    return fixed_count


def review_suspicious_prices(offers: List[Dict[str, Any]]) -> None:
    """Review offers with suspiciously high prices."""
    print("\nðŸ” REVIEWING SUSPICIOUS PRICES:")
    print("-" * 50)

    for i, offer in enumerate(offers):
        price = offer.get('price', '').strip()
        if price:
            try:
                # Extract numeric value
                numeric_price = float(price.replace('Ð»Ð².', '').replace('Ð»Ð²', '').replace('â‚¬', '').replace(',', '.').strip())
                if numeric_price > 10000:  # Suspiciously high threshold
                    print(f"[{i}] HIGH PRICE: {price}")
                    print(f"    Title: {offer.get('title', '')[:80]}")
                    print(f"    Link: {offer.get('link', '')}")
                    print()
            except ValueError:
                pass


async def main():
    """Main fix function."""
    json_path = "aratur.json"
    backup_path = "aratur_backup.json"

    print("ðŸ”§ ARATOUR ISSUES FIXER (Fresh Data Version)")
    print("=" * 50)

    # Load offers
    offers = load_offers(json_path)
    if not offers:
        return

    print(f"Loaded {len(offers)} offers from {json_path}")

    # Create backup
    save_offers(offers, backup_path)
    print(f"âœ“ Created backup: {backup_path}")

    # Fix offers by fetching fresh data
    print("\n1. Fetching fresh data from offer URLs...")
    fixed_count = await fix_offers_with_fresh_data(offers)
    print(f"   âœ“ Updated {fixed_count} offers with fresh data")

    # Review suspicious prices
    print("\n2. Reviewing suspicious prices...")
    review_suspicious_prices(offers)

    # Save fixed data
    save_offers(offers, json_path)

    print(f"\nâœ… FIX COMPLETE:")
    print(f"   - Offers updated with fresh data: {fixed_count}")
    print(f"   - Check suspicious prices manually")

    print("\nðŸ’¡ NEXT STEPS:")
    print("   - Review the suspicious prices manually")
    print("   - Re-run analyzer to verify fixes")


if __name__ == "__main__":
    asyncio.run(main())